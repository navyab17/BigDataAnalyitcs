{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c3f4439-0438-4d8d-b450-5d20f76a85eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://LAPTOP-A5D13KCQ.mshome.net:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v4.0.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=PySparkShell>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b4c2d7f0-135f-4f3b-8e75-1cbbcd2422b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = sc.textFile(\"BDA/students.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c8957e8e-037e-43a8-95f8-019133534b89",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o40.partitions.\n: org.apache.hadoop.ipc.RpcException: RPC response has invalid length of -16777216\r\n\tat org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1934)\r\n\tat org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1203)\r\n\tat org.apache.hadoop.ipc.Client$Connection.run(Client.java:1094)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m header \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfirst\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m rows \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mfilter(\u001b[38;5;28;01mlambda\u001b[39;00m line: line \u001b[38;5;241m!=\u001b[39m header)\n",
      "File \u001b[1;32mC:\\spark\\spark-4.0.1-bin-hadoop3\\spark-4.0.1-bin-hadoop3\\python\\pyspark\\core\\rdd.py:2755\u001b[0m, in \u001b[0;36mRDD.first\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2729\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfirst\u001b[39m(\u001b[38;5;28mself\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRDD[T]\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[0;32m   2730\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2731\u001b[0m \u001b[38;5;124;03m    Return the first element in this RDD.\u001b[39;00m\n\u001b[0;32m   2732\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2753\u001b[0m \u001b[38;5;124;03m    ValueError: RDD is empty\u001b[39;00m\n\u001b[0;32m   2754\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 2755\u001b[0m     rs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2756\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m rs:\n\u001b[0;32m   2757\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m rs[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32mC:\\spark\\spark-4.0.1-bin-hadoop3\\spark-4.0.1-bin-hadoop3\\python\\pyspark\\core\\rdd.py:2689\u001b[0m, in \u001b[0;36mRDD.take\u001b[1;34m(self, num)\u001b[0m\n\u001b[0;32m   2648\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2649\u001b[0m \u001b[38;5;124;03mTake the first num elements of the RDD.\u001b[39;00m\n\u001b[0;32m   2650\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2686\u001b[0m \u001b[38;5;124;03m[91, 92, 93]\u001b[39;00m\n\u001b[0;32m   2687\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2688\u001b[0m items: List[T] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m-> 2689\u001b[0m totalParts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetNumPartitions\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2690\u001b[0m partsScanned \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m   2692\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(items) \u001b[38;5;241m<\u001b[39m num \u001b[38;5;129;01mand\u001b[39;00m partsScanned \u001b[38;5;241m<\u001b[39m totalParts:\n\u001b[0;32m   2693\u001b[0m     \u001b[38;5;66;03m# The number of partitions to try in this iteration.\u001b[39;00m\n\u001b[0;32m   2694\u001b[0m     \u001b[38;5;66;03m# It is ok for this number to be greater than totalParts because\u001b[39;00m\n\u001b[0;32m   2695\u001b[0m     \u001b[38;5;66;03m# we actually cap it at totalParts in runJob.\u001b[39;00m\n",
      "File \u001b[1;32mC:\\spark\\spark-4.0.1-bin-hadoop3\\spark-4.0.1-bin-hadoop3\\python\\pyspark\\core\\rdd.py:819\u001b[0m, in \u001b[0;36mRDD.getNumPartitions\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    802\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mgetNumPartitions\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[0;32m    803\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    804\u001b[0m \u001b[38;5;124;03m    Returns the number of partitions in RDD\u001b[39;00m\n\u001b[0;32m    805\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    817\u001b[0m \u001b[38;5;124;03m    2\u001b[39;00m\n\u001b[0;32m    818\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 819\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jrdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpartitions\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msize()\n",
      "File \u001b[1;32mC:\\spark\\spark-4.0.1-bin-hadoop3\\spark-4.0.1-bin-hadoop3\\python\\lib\\py4j-0.10.9.9-src.zip\\py4j\\java_gateway.py:1362\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1356\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1357\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1358\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1359\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1361\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1362\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1363\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1365\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1366\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mC:\\spark\\spark-4.0.1-bin-hadoop3\\spark-4.0.1-bin-hadoop3\\python\\pyspark\\errors\\exceptions\\captured.py:282\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    279\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpy4j\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mprotocol\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Py4JJavaError\n\u001b[0;32m    281\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 282\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    283\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    284\u001b[0m     converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32mC:\\spark\\spark-4.0.1-bin-hadoop3\\spark-4.0.1-bin-hadoop3\\python\\lib\\py4j-0.10.9.9-src.zip\\py4j\\protocol.py:327\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    325\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    326\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 327\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    329\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    330\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    331\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    333\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o40.partitions.\n: org.apache.hadoop.ipc.RpcException: RPC response has invalid length of -16777216\r\n\tat org.apache.hadoop.ipc.Client$IpcStreams.readResponse(Client.java:1934)\r\n\tat org.apache.hadoop.ipc.Client$Connection.receiveRpcResponse(Client.java:1203)\r\n\tat org.apache.hadoop.ipc.Client$Connection.run(Client.java:1094)\r\n"
     ]
    }
   ],
   "source": [
    "header = data.first()\n",
    "rows = data.filter(lambda line: line != header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8863af08-4547-4413-964c-18a8295ad6da",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rows' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m split_rdd \u001b[38;5;241m=\u001b[39m \u001b[43mrows\u001b[49m\u001b[38;5;241m.\u001b[39mmap(\u001b[38;5;28;01mlambda\u001b[39;00m line: line\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'rows' is not defined"
     ]
    }
   ],
   "source": [
    "split_rdd = rows.map(lambda line: line.split(\",\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c05ae62-aaa4-43a5-a467-7a41cb703a5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Student Dataset (first 10 rows) ===\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'split_rdd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=== Student Dataset (first 10 rows) ===\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m \u001b[43msplit_rdd\u001b[49m\u001b[38;5;241m.\u001b[39mtake(\u001b[38;5;241m10\u001b[39m):\n\u001b[0;32m      3\u001b[0m  \u001b[38;5;28mprint\u001b[39m(row)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'split_rdd' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"=== Student Dataset (first 10 rows) ===\")\n",
    "for row in split_rdd.take(10):\n",
    " print(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884718e6-7692-47d5-9fb5-8f8cd54ff512",
   "metadata": {},
   "outputs": [],
   "source": [
    "students_rdd = split_rdd.map(lambda x: (int(x[0]), x[1], int(x[2]), x[3], int(x[4]), int(x[5]), int(x[6])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16fe1911-86a3-473b-a887-ec76137b9fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_marks_rdd = students_rdd.map(lambda x: (x[1], (x[4] + x[5] + x[6]) / 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9639857d-2ede-4958-a2a9-0520a8adee82",
   "metadata": {},
   "outputs": [],
   "source": [
    "passed_rdd = avg_marks_rdd.filter(lambda x: x[1] >= 75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef37734-ee35-474e-a41d-ade34dc09fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_passed_rdd = passed_rdd.sortBy(lambda x: x[1], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36356232-90d1-42a5-9f52-7df29230148f",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = sorted_passed_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7d626b-39d1-438d-8573-55b944bee2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Students with Average >= 75 ===\")\n",
    "for student in results:\n",
    " print(f\"Name: {student[0]}, Avg Marks: {student[1]:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eb6144f-d8c9-4fde-88d8-a3d28ac9ff40",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_passed = passed_rdd.count()\n",
    "print(\"\\nNumber of students who passed:\", count_passed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17828231-fa18-4c33-96bc-8e9e5af9716e",
   "metadata": {},
   "outputs": [],
   "source": [
    "topper = passed_rdd.reduce(lambda a, b: a if a[1] > b[1] else b)\n",
    "print(\"Topper:\", topper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07dd41aa-702c-4041-bde4-720f375415e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nFirst 5 Passed Students (via take):\")\n",
    "print(passed_rdd.take(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b251d0-7241-4cce-a6e4-4470c4e7df55",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
